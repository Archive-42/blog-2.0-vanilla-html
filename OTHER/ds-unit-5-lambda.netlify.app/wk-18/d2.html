<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>d2</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@algolia/algoliasearch-netlify-frontend@1/dist/algoliasearchNetlify.css" />
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@algolia/algoliasearch-netlify-frontend@1/dist/algoliasearchNetlify.js"></script>
<script type="text/javascript">
  algoliasearchNetlify({
    appId: 'THCDY2P71M',
    apiKey: 'e0c9e417bc883c22876055199e2db2eda847c35ce080f797902a0fe592b61790',
    siteId: '835ad7b5-37ce-4a32-824d-f3b0d3eab8e5',
    branch: 'master',
    selector: 'div#search',
  });
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/bgoonz/GIT-CDN-FILES/mdn-article.css"></head>
<body>
<h1 id="d2">D2</h1>
<p>{% embed url=“https://gist.github.com/bgoonz/3deac2c8f0dfedb503eefc78a4dba394” caption="" %}</p>
<h2 id="hash-tables">Hash Tables</h2>
<p>Hash tables (also known as hash maps) are associative arrays, or dictionaries, that allow for fast insertion, lookup and removal regardless of the number of items stored.</p>
<p>Here’s an example of how they can be used:</p>
<pre class="text"><code>HashTable phoneBook = new HashTable()
phoneBook.put(&quot;Adam&quot;, &quot;(202) 555-0309&quot;)
phoneBook.put(&quot;Beth&quot;, &quot;(335) 754-1215&quot;)

print(&quot;Adam’s number: &quot; + phoneBook.get(&quot;Adam&quot;))</code></pre>
<p>Internally they are similar to card indexes: An item can be found quickly by first jumping to its approximate location, and then searching locally from there.</p>
<p><img src="../.gitbook/assets/image.png" /></p>
<h3 id="representation">Representation</h3>
<p>The simplest way to implement a hash table is to use an <strong>array of linked lists</strong>.</p>
<p>Each array cell is called a <strong>bucket</strong>, and each list node stores a <strong>key-value pair</strong>.</p>
<p>Following the analogy from the previous section, the array cells that can be accessed quickly can be thought of as index cards, and nodes in the list as data cards.</p>
<p><img src="../.gitbook/assets/image%20%281%29.png" /></p>
<h3 id="finding-the-right-bucket">Finding the right bucket</h3>
<p>Which bucket a given key belongs to, is determined by a <strong>hash function</strong> as follows…</p>
<p><em>bucket index</em> = hash(<em>key</em>) % length(<em>bucket array</em>)</p>
<p>…where % denotes the remainder operator.</p>
<p>For efficiency reasons, an additional variable is used to keep track of the current total number of key-value pairs.☞</p>
<p>There are other ways to implement hash tables, but this is the representation that will be used here to introduce the subject. See section <a href="https://programming.guide/hash-tables.html#other-impl">Other Collision Resolution Strategies</a> for alternatives.</p>
<h2 id="insertion">Insertion</h2>
<p>Here’s how a mapping from key <code>k</code> to value <code>v</code> is inserted in a hash table:</p>
<p><strong>1.Rehash if load factor is too high. (Explained in separate section.)</strong></p>
<p>rehash_if_needed()</p>
<p><strong>2.Compute the hash for the key</strong></p>
<p>h = hash(k)</p>
<p><strong>3.Use remainder operator to compute the bucket index</strong></p>
<p>i = h % length(buckets)</p>
<p><strong>4.Look for key among existing nodes. If found update value of existing node.</strong></p>
<p>****</p>
<p>****</p>
<p>****</p>
<h3 id="lookup">Lookup</h3>
<p>Here’s how the value for a key <code>k</code> is retrieved from a hash table:1.Compute the hash for the keyh = hash(k)2.Use remainder operator to compute the bucket indexi = h % length(buckets)3.Search through bucket linearlyn = buckets[i]<br />
while n ≠ NULL:<br />
if n.key == k:<br />
return n.value<br />
n = n.next<br />
return NOT_FOUNDWrongkeyKeyfoundReturnvaluek,vk,vk,v</p>
<h3 id="remove">Remove</h3>
<p>Here’s how a key-value pair is removed from a hash table:1.Compute the hash for the keyh = hash(k)2.Use remainder operator to compute the bucket indexi = h % length(buckets)3.Search through bucket linearly to find keylast = NULL<br />
n = buckets[i]<br />
while n ≠ NULL:<br />
if n.key == k:<br />
if last == NULL:<br />
buckets[i] = n.next<br />
else:<br />
last.next = n.next<br />
n = n.next</p>
<h3 id="rehashing">Rehashing</h3>
<p>As the lengths of the linked lists grow, the average lookup time increases.</p>
<p>To keep the linked lists short and lookups fast, the number of buckets must be increased (and nodes redistributed). This is called <strong>rehashing</strong>.1.Allocate a new bucket array.new_size = 2 * length(buckets)<br />
new_buckets = new Bucket[new_size]2.Reinsert all elements in new array.for i in 0..buckets.length - 1:<br />
while buckets[i] ≠ NULL:<br />
n = buckets[i]<br />
buckets[i] = n.next<br />
h = hash(n.k)<br />
i = h % new_length<br />
n.next = new_buckets[i]<br />
new_buckets[i] = n<br />
3.Replace old array with new arraybuckets = new_bucketsbucketsnew_buckets</p>
<p>During rehashing the number of buckets is increased by some factor, typically 2. As shown in the article <a href="https://programming.guide/hash-tables-complexity.html">Hash Tables: Complexity</a>, it is important that the growth is exponential.</p>
<h3 id="traversal">Traversal</h3>
<p>1.For each bucket…for i in 0..buckets.length - 1:2.…traverse each node n = buckets[i]:<br />
while n ≠ NULL:<br />
process(n)<br />
n = n.next</p>
<p>Note that the order in which the key-value pairs are traversed is <strong>unpredictable</strong> due to the nature of hashing and rehashing.</p>
<h3 id="load-factor-and-capacity">Load Factor and Capacity</h3>
<p>The <strong>load factor</strong> is the average number of key-value pairs per bucket.load factor=total number of key-value pairsnumber of buckets</p>
<p>It is when the load factor reaches a given limit that rehashing kicks in. Since rehashing increases the number of buckets, it reduces the load factor.</p>
<p>The load factor limit is usually configurable and offers a <strong>tradeoff between time and space costs</strong>.Lower limitMore empty bucketsMore memory overheadHigher limitLarger bucketsSlower lookups</p>
<p>The default load factor for a Java <a href="https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html"><code>HashMap</code></a> is 0.75 and for a C# <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8"><code>Hashtable</code></a> it’s 1.0.</p>
<p>The <strong>capacity</strong> is the maximum number of key-value pairs for the given load factor limit and current bucket count.capacity=number of buckets×load factor limit</p>
<p>Since rehashing increases the number of buckets, it increases the capacity.</p>
<p>The default initial capacity for a Java <a href="https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html"><code>HashMap</code></a> is 12 and for a C# <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8"><code>Hashtable</code></a> it’s 0, i.e. the bucket array is initialized lazily upon first insertion.</p>
<p><strong>Example:</strong> Here’s the structure of a hash table, configured with load factor limit of 4.Currentload factor:24 / 8 = 3Configuredlimit:4Current capacity:8 × 4 = 32</p>
<h3 id="complexity-analysis">Complexity Analysis</h3>
<p>As is clear from the way insert, lookup and remove works, the run time is proportional to the length of the linked lists. In worst case all keys hash to the same bucket, i.e. the whole data structure becomes equivalent to a linked list. This means that all operations run in <em>O</em>(<em>n</em>).</p>
<p>Assuming however that keys are dispersed evenly among the buckets, it can be shown that the complexity of the expected run time is <em>O</em>(1) for all operations.</p>
<p>See article <a href="https://programming.guide/hash-tables-complexity.html">Hash Tables: Complexity</a> for a detailed analysis.</p>
<h3 id="other-collision-resolution-strategies">Other Collision Resolution Strategies <a id="other-impl"></a></h3>
<p>When two keys hash to the same bucket, i.e. whenhash(_k_1) ≡ hash(_k_2) (mod number of buckets)</p>
<p>it’s called a <strong>collision</strong>. The collision handling strategy described so far (one linked list per bucket) is an example of <strong>closed addressing</strong> using separate chains. Instead of linked lists, one can also use binary search trees, or as in the case of Java 9, a linked list up to a certain limit, and then convert it to a BST when more elements are added.</p>
<p>The alternative, <strong>open addressing</strong>, is to store all key-value pairs directly in the hash table array, i.e. there’s at most one element per bucket. (The size of the array must always be at least as large as the number of elements stored.)</p>
<h2 id="hash-tables-complexity">Hash Tables: Complexity</h2>
<p>This article is written with separate chaining and closed addressing in mind, specifically implementations based on <strong>arrays of linked lists</strong>. Most of the analysis however applies to other techniques, such as basic open addressing implementations.☞</p>
<p>Only operations that scale with the number of elements <em>n</em> are considered in the analysis below. In particular, the hash function is assumed to run in constant time.</p>
<h3 id="length-of-chains">Length of chains</h3>
<p>As is clear from the way lookup, insert and remove works, the run time is proportional to the number of keys in the given chain. So, to analyze the complexity, we need to analyze the length of the chains.</p>
<h4 id="worst-case">Worst Case</h4>
<p>If we’re unlucky with the keys we encounter, or if we have a poorly implemented hash function, all keys may hash to the same bucket.</p>
<p>This means that the <strong>worst-case complexity of a hash table is the same as that of a linked list</strong>: <em>O</em>(<em>n</em>) for insert, lookup and remove.</p>
<p>This is however a pathological situation, and the theoretical worst-case is often uninteresting in practice. When discussing complexity for hash tables the focus is usually on <strong>expected</strong> run time.</p>
<h4 id="uniform-hashing">Uniform Hashing</h4>
<p>The expected length of any given linked list depends on how the hash function spreads out the keys among the buckets. For the purpose of this analysis, we will assume that we have an ideal hash function. This is a common assumption to make. So common in fact, that it has a name:</p>
<h4 id="simple-uniform-hashing-assumption">Simple Uniform Hashing Assumption</h4>
<p>In a hash table with <em>m</em> buckets, each key is hashed to any given bucket…</p>
<ul>
<li>…with the same probability, 1 / <em>m</em></li>
<li>…independently of which bucket any other key is hashed to.</li>
</ul>
<p>With SUHA the keys are distributed <strong>uniformly</strong> and the expected length of any given linked list is therefore <em>n</em> / <em>m</em>.</p>
<h4 id="the-magic-part">The Magic Part</h4>
<p>As you may recall, the <em>n</em> / <em>m</em> ratio is called the load factor, and that rehashing guarantees that this is bound by the configured load factor limit. (See <a href="https://programming.guide/hash-table-load-factor-and-capacity.html">Hash Table Load Factor and Capacity</a>.) Since the load factor limit is constant, <strong>the expected length of all chains can be considered constant</strong>.☞</p>
<p><strong>A common misconception</strong> is that SUHA implies constant time worst case complexity. SUHA however, does not say that all keys <em>will</em> be distributed uniformly, only that the probability distribution is uniform. Even with a uniform probability, it is still <em>possible</em> for all keys to end up in the same bucket, thus worst case complexity is still linear.</p>
<h3 id="insert">Insert</h3>
<p>An insertion will search through one bucket linearly to see if the key already exists. This runs in <em>O</em>(<em>n</em> / <em>m</em>) which we know from the previous section is <em>O</em>(1). If the key is found, a value is updated, if not, a new node is appended to the list. Regardless of which, this part is in <em>O</em>(1).</p>
<p>If we’re unlucky, rehashing is required before all that. Since rehashing performs <em>n</em> constant time insertions, it runs in Θ(<em>n</em>).</p>
<p>That being said, rehashes are rare. In fact, they are so rare that <em>in average</em> insertion still runs in constant time. We say that the <strong>amortized time complexity for insert is</strong> <em><strong>O</strong></em><strong>(1)</strong>.</p>
<p><strong>Proof:</strong> Suppose we set out to insert <em>n</em> elements and that rehashing occurs at each power of two. Let’s assume also that <em>n</em> is a power of two so we hit the worst case scenario and have to rehash on the very last insertion.</p>
<p>We would have to rehash after inserting element 1, 2, 4, …, <em>n</em>. Since each rehashing reinserts all current elements, we would do, in total, 1 + 2 + 4 + 8 + … + <em>n</em> = 2n − 1 extra insertions due to rehashing. In other words, all rehashing necessary incurs an average overhead of less than 2 extra insertions per element.</p>
<p>We conclude that despite the growing cost of rehashing, the average number of insertions per element stays constant.</p>
<h3 id="lookup-1">Lookup</h3>
<p>A lookup will search through the chain of one bucket linearly. This is in <em>O</em>(<em>n</em> / <em>m</em>) which, again, is <em><strong>O</strong></em><strong>(1)</strong>.</p>
<h3 id="removal">Removal</h3>
<p>A removal will search through one bucket linearly. If the key is found, it is “unlinked” in constant time, so remove runs in <em><strong>O</strong></em><strong>(1)</strong> as well.</p>
<p>If one wants to reclaim unused memory, removal may require allocating a smaller array and rehash into that. In this case removal runs in <em>O</em>(<em>n</em>) in worst case, and <em>O</em>(1) amortized.</p>
<h3 id="traversal-1">Traversal</h3>
<p>There’s no way to know which buckets are empty, and which ones are not, so all buckets must be traversed. This means traversal is Θ(<em>n</em> + <em>m</em>).</p>
<p>In practice this is only relevant if the hash table is initialized with a very large capacity. After the first rehashing the number of buckets can be considered linearly proportional to the number of items, and traversal is <em>Θ</em>(<em>n</em>).</p>
<h2 id="inked-hash-table">inked Hash Table</h2>
<p>A linked hash table is a <strong>combination of a hash table and a linked list</strong>. Nodes are arranged in buckets but also have a doubly linked list running through them.</p>
<p>One can use the hash table structure for fast hash based lookups, and the linked list for effirient traversal.</p>
<h3 id="hash-table-traversal">Hash Table Traversal</h3>
<p>There’s no way to know which buckets are empty, and which ones are not, so all buckets must be traversed. This means traversal is Θ(<em>n</em> + <em>m</em>).</p>
<p>In practice this is only relevant if the hash table is initialized with a very large capacity. After the first rehashing the number of buckets can be considered linearly proportional to the number of items, and traversal is <em>Θ</em>(<em>n</em>).</p>
<h3 id="adding-a-linked-list">Adding a linked list</h3>
<p>If one wants to avoid the overhead due to empty buckets one can let a linked list run through all nodes. Whenever a node is added to the hash table, it’s appended to this list.</p>
<p><strong>Example:</strong> A linked hash table based on separate chaining.</p>
<p>This effectively trades some memory (additional next / prev pointers) for improved speed.</p>
<p>Since the overlay list structure is doubly linked, append and remove are constant time operations, so it does not affect the complexity of other operations.</p>
<p>The <a href="https://docs.oracle.com/javase/8/docs/api/java/util/LinkedHashMap.html"><code>LinkedHashMap</code></a> in the Java API is an example of this technique.</p>
</body>
</html>
